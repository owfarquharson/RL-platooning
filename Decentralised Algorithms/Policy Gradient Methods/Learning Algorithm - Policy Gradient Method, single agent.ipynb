{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Method: Learning Algorithm - Single agent\n",
    "\n",
    "This script contains the algorithm that uses PGM for improving the controller directly applied to the single agent system \n",
    "\n",
    "### 'Continuous_car' \n",
    "\n",
    "Instantiated with a controller K, and boolean variables defining whether you want system noise or controller noise\n",
    "\n",
    "### 'PGM'\n",
    "\n",
    "This carries out a chosen number of updates to the controller given an initial controller and an update constant, epsilon.\n",
    "It returns a list of controllers learned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Continuous_car():\n",
    "\n",
    "    \n",
    "    def __init__(self,K,noisy_model = False, noisy_controller=False):\n",
    "        \n",
    "        #system\n",
    "        self.A = np.array([[1,1],[0, 1]])\n",
    "        #x = np.array([[1],[1]])\n",
    "        self.B = np.array([[0.5],[1]])\n",
    "\n",
    "        #cost matrices\n",
    "        self.E = np.array([[1,0],[0, 0.5]])\n",
    "        self.F = np.array([1])\n",
    "\n",
    "        # Arbitrary gain choice used (stability checked)\n",
    "        self.K = K\n",
    "        \n",
    "        # Optimal Gains found using LQR on Matlab:\n",
    "        # K_Optimal = [ 0.4634, 1.0170 ]\n",
    "\n",
    "        self.disc_fact = 0.99\n",
    "        self.n_states = 2\n",
    "        \n",
    "        if noisy_model == True:\n",
    "            self.sigma_model = 0.25\n",
    "        else:\n",
    "            self.sigma_model = 0\n",
    "            \n",
    "        self.sigma_controller = 0.2\n",
    "        self.noisy_controller = noisy_controller\n",
    "    \n",
    "    def GetPolicyInput(self,x):\n",
    "        #For a given state, return the input u according to a defined policy\n",
    "        # x: 2x1 array\n",
    "        # K will be a 1x2 array\n",
    "        \n",
    "        inp = np.matmul(self.K,x)[0] #scalar\n",
    "        \n",
    "        if self.noisy_controller:\n",
    "            inp += self.GetControllerNoise() #scalar\n",
    "        \n",
    "        return inp #scalar\n",
    "    \n",
    "    def GetControllerNoise(self):\n",
    "        # returns scalar value of noise\n",
    "        \n",
    "        contr_noise = self.sigma_controller * np.random.randn(1)[0] # scalar\n",
    "        \n",
    "        return contr_noise\n",
    "    \n",
    "    def GetCost(self, x, u):\n",
    "        #For a given state, return the one step cost of this new state\n",
    "        \n",
    "        # x is a 2x1 array\n",
    "        # u is a scalar\n",
    "        \n",
    "        # x'Ex\n",
    "        cost1 = np.matmul(np.matmul(x.transpose(),self.E),x)[0][0]\n",
    "        \n",
    "        # u'Fu\n",
    "        cost2 = u*u*self.F[0]\n",
    "        \n",
    "        return cost1+cost2 #scalar\n",
    "        \n",
    "    def GetNoise(self):\n",
    "        # Returns a vector with noise for the model only for velocity state\n",
    "        \n",
    "        w = np.array([0,self.sigma_model*np.random.randn(1)[0]]).reshape(2,1)\n",
    "        \n",
    "        return w\n",
    "        #return np.array([[0],[0]])\n",
    "    \n",
    "    def GetNextState(self,current_state,current_input):\n",
    "            \n",
    "        \n",
    "        x_next_1 = np.matmul(self.A,current_state)\n",
    "        \n",
    "        x_next_2 = self.B * current_input\n",
    "        x_next_3 = self.GetNoise()\n",
    "        #pdb.set_trace()\n",
    "        x_next = x_next_1 + x_next_2 + x_next_3\n",
    "        return x_next\n",
    "    \n",
    "\n",
    "    def RunEpisode(self, episode_length, state_initial):\n",
    "        #function will return lists of the states, inputs and costs for a trajectory given an \n",
    "        #initial state and f, a probability of the input being randomly generated\n",
    "        '''\n",
    "        length: integer\n",
    "        state_initial: list form, e.g. [3,2] for position of 3 and velocity of 2\n",
    "        '''\n",
    "\n",
    "        x = np.array(state_initial).reshape(2,1)\n",
    "\n",
    "        state_list = [x]\n",
    "        cost_list = []\n",
    "        input_list = []\n",
    "        pos_list = [x[0][0]]\n",
    "        vel_list = [x[1][0]]\n",
    "            \n",
    "\n",
    "\n",
    "       \n",
    "        #input_list.append(sys.GetInput(x))\n",
    "\n",
    "        for k in range(episode_length):\n",
    "            \n",
    "            u = self.GetPolicyInput(x)\n",
    "                \n",
    "\n",
    "            input_list.append(u)\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "\n",
    "            x = self.GetNextState(x,u)\n",
    "            cost = self.GetCost(x,u)\n",
    "\n",
    "            state_list.append(x)\n",
    "            cost_list.append(cost)\n",
    "\n",
    "\n",
    "            pos_list.append(x[0][0])\n",
    "            vel_list.append(x[1][0])\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "\n",
    "        return state_list, cost_list, input_list, pos_list, vel_list\n",
    "    \n",
    "    \n",
    "    def CollectSimulations(self,number_of_episodes=10,episode_length=50):\n",
    "        # This function runs a certain number of episodes and returns them all in big matrix lists of lists\n",
    "        \n",
    "        Vel_Matrix_list = []\n",
    "        Pos_Matrix_list = []\n",
    "        \n",
    "        U_Matrix_list = []\n",
    "        Cost_Matrix_list = []\n",
    "        \n",
    "        ## define some initial states parameters\n",
    "        \n",
    "        sigma_pos = 4\n",
    "        sigma_vel = 2\n",
    "        \n",
    "        def GetRandomInitialState():\n",
    "            \n",
    "            x_in = sigma_pos * np.random.randn(1)[0]\n",
    "            v_in = sigma_vel * np.random.randn(1)[0]\n",
    "            \n",
    "            return [x_in,v_in]\n",
    "        \n",
    "        for episode in range(number_of_episodes):\n",
    "            \n",
    "            #Get Initial State\n",
    "            X_initial = GetRandomInitialState()\n",
    "            \n",
    "            SL,CO,IN,PO,VEL = self.RunEpisode(episode_length,X_initial)\n",
    "            \n",
    "            Vel_Matrix_list.append(VEL)\n",
    "            Pos_Matrix_list.append(PO)\n",
    "            U_Matrix_list.append(IN)\n",
    "            Cost_Matrix_list.append(CO)\n",
    "        \n",
    "        return Pos_Matrix_list,Vel_Matrix_list, U_Matrix_list,Cost_Matrix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGradLnPi(x_k, u_k, K, sig):\n",
    "    # takes in the following values:\n",
    "    \n",
    "    #x_k - the current state vector (2x1)\n",
    "    #u_k - the current input value (scalar)\n",
    "    \n",
    "    #K - the current policy we are following (in the form u=Kx) FOR ONLY ONE INPUT - IE k IS A 1X2 MATRIX\n",
    "    #sig - the value of the controller noise sigma (scalar)\n",
    "    \n",
    "    # a 2x1 vector\n",
    "    return (1/sig) * (u_k - np.matmul(K,x_k)) * x_k\n",
    "\n",
    "    \n",
    "def GetValFunc(x_k,W):\n",
    "    # function that takes in a vector of the current state x_k and the current weights W and returns the scalar value function\n",
    "    # W is a 4x1 array\n",
    "    \n",
    "    position = x_k[0][0]\n",
    "    velocity = x_k[1][0]\n",
    "    \n",
    "    valfunc = position*position * W[0][0] + position * velocity * W[1][0] + velocity * velocity * W[2][0] + W[3][0]\n",
    "    \n",
    "    return valfunc  #scalar\n",
    "\n",
    "def GetGradV(x_k):\n",
    "    #function that returns a 4x1 vector of the gradient of the value function\n",
    "    \n",
    "    position = x_k[0][0]\n",
    "    velocity = x_k[1][0]\n",
    "    \n",
    "    return np.array([position*position, position*velocity, velocity*velocity,1]).reshape(4,1)\n",
    "    \n",
    "\n",
    "\n",
    "## We will do this first one by using Gt as a Monte Carlo sample. Ie we collect one trajectory and then for each transition within this,\n",
    "# we update the Theta based on the actual observed returns \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PGM(K_initial,epsilon,n_episodes=1000,ep_length=500):\n",
    "        \n",
    "    #rand_ep = np.random.randint(0,n_episodes)\n",
    "    #print(f'random episode = {rand_ep}')\n",
    "    #rand_transition = np.random.randint(0,ep_length)\n",
    "    #print(f'random transition = {rand_transition}')\n",
    "\n",
    "\n",
    "\n",
    "    # Initialise Controller\n",
    "\n",
    "    K = K_initial\n",
    "    \n",
    "\n",
    "\n",
    "    list_of_controllers = []\n",
    "\n",
    "\n",
    "    # for however many iterations:\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        ten_perc = n_episodes/10\n",
    "        if episode%ten_perc == 0:\n",
    "            print(f'{episode} done, {n_episodes-episode} remaining!')\n",
    "\n",
    "        # Create the new gain matrix\n",
    "        list_of_controllers.append(K)\n",
    "\n",
    "        # Generate an episode (we need states, actions, costs at every step)\n",
    "\n",
    "        sys = Continuous_car(K,noisy_model = True, noisy_controller=True)\n",
    "        X1,V1,U,C = sys.CollectSimulations(1,ep_length)\n",
    "\n",
    "        #make the arrays 1d\n",
    "        X1 = X1[0]\n",
    "        V1 = V1[0]\n",
    "        U = U[0]\n",
    "        C = C[0]\n",
    "\n",
    "        K_this_ep = K.copy()\n",
    "        # for each transition, we do an update:\n",
    "\n",
    "        for transition_idx, _ in enumerate(U):\n",
    "        #for transition_idx in range(1):\n",
    "\n",
    "\n",
    "\n",
    "            # calculate discounted return from this point onwards\n",
    "            G = 0\n",
    "            k = transition_idx\n",
    "            while True:\n",
    "                G += sys.disc_fact **(k-transition_idx) * C[k]                    \n",
    "\n",
    "                if k==len(C)-1:\n",
    "                    break\n",
    "\n",
    "                k +=1\n",
    "            #G = G/(k+1-transition_idx)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # get the value of grad(ln(pi)) evaluated at our sampled action and state\n",
    "\n",
    "            x1_k = X1[transition_idx]\n",
    "            v1_k = V1[transition_idx]\n",
    "            \n",
    "            u_k = U[transition_idx]\n",
    "            sig = sys.sigma_controller\n",
    "\n",
    "            x_k = np.array([x1_k,v1_k]).reshape(2,1)\n",
    "\n",
    "            ######## Use most recently updated K ##############\n",
    "            \n",
    "            gradJ = GetGradLnPi(x_k, u_k, K_this_ep, sig).reshape(1,2)\n",
    "\n",
    "            K_this_ep = (K_this_ep - epsilon * (sys.disc_fact**transition_idx) *  G * gradJ)[0]  #term removed\n",
    "            #pdb.set_trace()\n",
    "\n",
    "\n",
    "        K = K_this_ep\n",
    "\n",
    "    list_of_controllers.append(K)\n",
    "    \n",
    "    print('All Improvements Done!')\n",
    "\n",
    "    return list_of_controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
